{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d867db",
   "metadata": {},
   "source": [
    "# Practice Assignment: Wine Classification (Starter Notebook)\n",
    "\n",
    "This notebook is a **starter template** with **TODOs**.  \n",
    "Complete each section and keep your answers in markdown cells.\n",
    "\n",
    "**Dataset:** `sklearn.datasets.load_wine`  \n",
    "**Goal:** Predict the wine class (3 classes) from 13 numeric features.\n",
    "\n",
    "---\n",
    "\n",
    "## Rules\n",
    "- Do **not** change the dataset.\n",
    "- Use a fixed random seed where requested.\n",
    "- Write short answers where asked.\n",
    "- Keep your code clean and reproducible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1150a0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run this cell.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Models you may use (you will pick at least 3 in Part 4)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37928595",
   "metadata": {},
   "source": [
    "## Part 0 — Load the dataset\n",
    "\n",
    "**TODOs**\n",
    "1. Load the dataset using `load_wine()`\n",
    "2. Create a DataFrame `df` containing the features\n",
    "3. Create `X` (features) and `y` (target)\n",
    "4. Print:\n",
    "   - dataset shape\n",
    "   - feature names\n",
    "   - class distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fb85c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load wine dataset\n",
    "wine = load_wine()\n",
    "\n",
    "# TODO: Create DataFrame with features\n",
    "# Hint: wine.data is a (n_samples, n_features) numpy array\n",
    "# Hint: wine.feature_names is the list of column names\n",
    "df = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "\n",
    "# TODO: Create X and y\n",
    "X = df\n",
    "y = wine.target  # 0,1,2 correspond to classes\n",
    "\n",
    "# TODO: Print dataset shape\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", np.shape(y))\n",
    "\n",
    "# TODO: Print feature names\n",
    "print(\"Features:\", list(X.columns))\n",
    "\n",
    "# TODO: Print class distribution\n",
    "# Hint: np.bincount(y)\n",
    "print(\"Class counts:\", np.bincount(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06502cf",
   "metadata": {},
   "source": [
    "## Part 1 — Quick EDA (minimal but required)\n",
    "\n",
    "**TODOs**\n",
    "1. Check for missing values\n",
    "2. Show basic descriptive statistics (mean, std, min, max)\n",
    "3. Plot at least **one** feature distribution grouped by class\n",
    "\n",
    "**Answer in markdown:**\n",
    "1. Are all features on similar numeric scales?\n",
    "2. Do any features appear clearly class-separating?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722034a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Missing values check\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "# TODO: Basic descriptive statistics\n",
    "# Hint: df.describe().T\n",
    "display(df.describe().T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088e28a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot one feature distribution grouped by class\n",
    "# Pick ONE feature name from df.columns, e.g. 'alcohol' or 'color_intensity'\n",
    "feature = \"alcohol\"  # TODO: change if you want\n",
    "\n",
    "plt.figure()\n",
    "for cls in np.unique(y):\n",
    "    plt.hist(df.loc[y == cls, feature], alpha=0.5, bins=15, label=f\"class {cls}\")\n",
    "plt.xlabel(feature)\n",
    "plt.ylabel(\"count\")\n",
    "plt.title(f\"Distribution of {feature} by class\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15591c5",
   "metadata": {},
   "source": [
    "**Your answers (TODO):**\n",
    "\n",
    "1. Feature scales:  \n",
    "   - TODO\n",
    "\n",
    "2. Clear class separation:  \n",
    "   - TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6d6853",
   "metadata": {},
   "source": [
    "## Part 2 — Train/test split\n",
    "\n",
    "Use an **80/20** split and `random_state=42`.\n",
    "\n",
    "**TODOs**\n",
    "- Create `X_train, X_test, y_train, y_test`\n",
    "- Print their shapes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29de1e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_test :\", X_test.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"y_test :\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538a49d9",
   "metadata": {},
   "source": [
    "## Part 3 — Baseline model (Logistic Regression, **no scaling**)\n",
    "\n",
    "Train Logistic Regression without scaling and evaluate.\n",
    "\n",
    "**TODOs**\n",
    "1. Train Logistic Regression\n",
    "2. Predict on test set\n",
    "3. Report:\n",
    "   - Accuracy\n",
    "   - Confusion matrix\n",
    "   - Classification report\n",
    "\n",
    "**Reflection (markdown):**\n",
    "- Why might this baseline be suboptimal even if accuracy looks OK?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb30e292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Baseline logistic regression WITHOUT scaling\n",
    "# Hint: increase max_iter to avoid convergence warnings\n",
    "baseline_clf = LogisticRegression(max_iter=5000)\n",
    "\n",
    "baseline_clf.fit(X_train, y_train)\n",
    "y_pred_base = baseline_clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_base))\n",
    "print(\"\\nConfusion matrix:\\n\", confusion_matrix(y_test, y_pred_base))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred_base))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cde7ab",
   "metadata": {},
   "source": [
    "**Reflection (TODO):**  \n",
    "- TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d200d8",
   "metadata": {},
   "source": [
    "## Part 4 — Scaling + Logistic Regression (core lesson)\n",
    "\n",
    "Now apply `StandardScaler` and retrain Logistic Regression using a Pipeline.\n",
    "\n",
    "**TODOs**\n",
    "1. Create a pipeline: `StandardScaler()` -> `LogisticRegression()`\n",
    "2. Fit on training set\n",
    "3. Evaluate on test set\n",
    "4. Fill the comparison table (Accuracy and Macro F1)\n",
    "\n",
    "**Required written explanation (markdown):**\n",
    "- Explain *why* scaling changed (or did not change) the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29949d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Logistic regression WITH scaling using a pipeline\n",
    "scaled_lr = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(max_iter=5000))\n",
    "])\n",
    "\n",
    "scaled_lr.fit(X_train, y_train)\n",
    "y_pred_scaled = scaled_lr.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_scaled))\n",
    "print(\"\\nConfusion matrix:\\n\", confusion_matrix(y_test, y_pred_scaled))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred_scaled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34edddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute Macro F1 for baseline and scaled versions and fill the table below\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "acc_base = accuracy_score(y_test, y_pred_base)\n",
    "f1_base = f1_score(y_test, y_pred_base, average=\"macro\")\n",
    "\n",
    "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "f1_scaled = f1_score(y_test, y_pred_scaled, average=\"macro\")\n",
    "\n",
    "acc_base, f1_base, acc_scaled, f1_scaled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069ebd89",
   "metadata": {},
   "source": [
    "### Comparison table (fill in the numbers)\n",
    "\n",
    "| Metric | No Scaling (baseline LR) | With Scaling (LR) |\n",
    "|---|---:|---:|\n",
    "| Accuracy | TODO | TODO |\n",
    "| Macro F1 | TODO | TODO |\n",
    "\n",
    "**Explanation (TODO):**  \n",
    "- TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f429d966",
   "metadata": {},
   "source": [
    "## Part 5 — Model comparison (train at least 3 models)\n",
    "\n",
    "Train and evaluate **at least 3** models from the list below:\n",
    "\n",
    "- KNN\n",
    "- SVM (linear or RBF)\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "\n",
    "**Rules**\n",
    "- Use the same train/test split.\n",
    "- Use scaling for models that need it (KNN, SVM, Logistic Regression).\n",
    "- Keep default hyperparameters (you may set `random_state` where available).\n",
    "\n",
    "**TODOs**\n",
    "1. Train your chosen models\n",
    "2. Compute accuracy + macro F1 for each\n",
    "3. Build a comparison table (DataFrame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6397a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define candidate models\n",
    "# Tip: use Pipelines for models that need scaling\n",
    "models = {\n",
    "    \"KNN (scaled)\": Pipeline([(\"scaler\", StandardScaler()), (\"clf\", KNeighborsClassifier())]),\n",
    "    \"SVM RBF (scaled)\": Pipeline([(\"scaler\", StandardScaler()), (\"clf\", SVC())]),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "}\n",
    "\n",
    "# TODO: Choose at least 3 models to evaluate\n",
    "chosen_model_names = [\n",
    "    # \"KNN (scaled)\",\n",
    "    # \"SVM RBF (scaled)\",\n",
    "    # \"Decision Tree\",\n",
    "    # \"Random Forest\",\n",
    "]\n",
    "\n",
    "# TODO: Evaluate chosen models and store results\n",
    "results = []\n",
    "\n",
    "for name in chosen_model_names:\n",
    "    clf = models[name]\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    results.append({\n",
    "        \"model\": name,\n",
    "        \"accuracy\": accuracy_score(y_test, preds),\n",
    "        \"macro_f1\": f1_score(y_test, preds, average=\"macro\"),\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"macro_f1\", ascending=False)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b72901",
   "metadata": {},
   "source": [
    "**TODO:** Briefly comment (1–3 sentences) on which model performed best and any surprises.\n",
    "\n",
    "- TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93854aad",
   "metadata": {},
   "source": [
    "## Part 6 — Overfitting check\n",
    "\n",
    "Pick your best-performing model from Part 5 and compare:\n",
    "\n",
    "- Training accuracy\n",
    "- Test accuracy\n",
    "\n",
    "**TODOs**\n",
    "1. Select the best model name from `results_df`\n",
    "2. Refit it on training set (if needed)\n",
    "3. Compute training and test accuracy\n",
    "4. Answer: Is there evidence of overfitting?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4daa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Pick best model from results_df\n",
    "# Example:\n",
    "# best_model_name = results_df.iloc[0][\"model\"]\n",
    "best_model_name = None  # TODO\n",
    "\n",
    "best_clf = models[best_model_name]\n",
    "best_clf.fit(X_train, y_train)\n",
    "\n",
    "train_acc = accuracy_score(y_train, best_clf.predict(X_train))\n",
    "test_acc = accuracy_score(y_test, best_clf.predict(X_test))\n",
    "\n",
    "print(\"Best model:\", best_model_name)\n",
    "print(\"Training accuracy:\", train_acc)\n",
    "print(\"Test accuracy    :\", test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabd4cf0",
   "metadata": {},
   "source": [
    "**Overfitting judgment (TODO):**  \n",
    "- TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6243a364",
   "metadata": {},
   "source": [
    "## Part 7 — Final reflection (required)\n",
    "\n",
    "Answer briefly:\n",
    "\n",
    "1. Which model performed best and why (in your view)?\n",
    "2. Which model would you **not** trust for this dataset?\n",
    "3. One common beginner mistake you made or almost made.\n",
    "\n",
    "**Your answers (TODO):**\n",
    "1. TODO  \n",
    "2. TODO  \n",
    "3. TODO  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebb48e8",
   "metadata": {},
   "source": [
    "## Optional stretch goals (extra)\n",
    "\n",
    "Pick any ONE:\n",
    "\n",
    "- Add 5-fold cross-validation and compare to your test-set result\n",
    "- Make a PCA 2D plot to visualize class separation\n",
    "- Try GridSearchCV on KNN (`n_neighbors`) or SVM (`C`, `gamma`) *carefully* (small grid)\n",
    "\n",
    "*(Optional work should be clearly separated from the main assignment.)*\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
