{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "147d8cb2",
   "metadata": {},
   "source": [
    "# ðŸ§  Session 6: Dimensionality Reduction & Wrap-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bb21f8",
   "metadata": {},
   "source": [
    "## ðŸ•’ 00:00â€“00:15 â€” PCA Explained\n",
    "**Principal Component Analysis (PCA)** is a method for reducing the dimensionality of a dataset while preserving as much variability (information) as possible.\n",
    "\n",
    "**Why use PCA?**\n",
    "- To reduce noise\n",
    "- To visualize high-dimensional data\n",
    "- To improve performance of ML models\n",
    "\n",
    "**Steps in PCA:**\n",
    "1. Standardize the data\n",
    "2. Compute covariance matrix\n",
    "3. Compute eigenvectors/eigenvalues\n",
    "4. Select top-k principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18f5ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset (without labels)\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "iris = load_iris(as_frame=True)\n",
    "X = iris.data\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b06118c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to reduce to 2 dimensions\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d78da59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PCA result\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1])\n",
    "plt.title('PCA Projection of Iris Data')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73247345",
   "metadata": {},
   "source": [
    "## ðŸ•’ 00:15â€“00:25 â€” Anomaly Detection Basics\n",
    "**Anomaly Detection** is the process of identifying data points that deviate significantly from the majority.\n",
    "\n",
    "**When is it useful?**\n",
    "- Fraud detection\n",
    "- Server monitoring\n",
    "- Defect detection\n",
    "\n",
    "Common algorithms:\n",
    "- Isolation Forest\n",
    "- Local Outlier Factor (LOF)\n",
    "- One-Class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae66a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Try Isolation Forest on Iris data\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "clf = IsolationForest(contamination=0.05, random_state=42)\n",
    "outliers = clf.fit_predict(X)\n",
    "\n",
    "# Visualize anomaly scores\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=outliers, cmap='coolwarm')\n",
    "plt.title('Anomaly Detection using Isolation Forest')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734cf2ce",
   "metadata": {},
   "source": [
    "## ðŸ•’ 00:25â€“00:40 â€” Hands-On: PCA + Clustering (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713af920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply KMeans after PCA (just for fun)\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_pca)\n",
    "\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='Set1')\n",
    "plt.title('Clustering on PCA-Reduced Iris Data')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd81fae",
   "metadata": {},
   "source": [
    "## ðŸ•’ 00:40â€“00:45 â€” Recap & Exit Ticket\n",
    "- âœ… PCA helps visualize and simplify data\n",
    "- âœ… Anomalies = rare but important data points\n",
    "- âœ… Combining techniques = better insight\n",
    "\n",
    "**Reflection Prompt:**\n",
    "What concept today was most surprising or useful to you?\n",
    "\n",
    "**Mini Quiz:**\n",
    "1. What does PCA aim to preserve?\n",
    "2. Name one use case of anomaly detection.\n",
    "3. Why reduce dimensions before clustering?"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
